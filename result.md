| backbone channels | head channels | pretrain | KD   | block    | LaSOT            | param  | macs  | speed | speed(1thread) |
| ----------------- | ------------- | -------- | ---- | -------- | ---------------- | ------ | ----- | ----- | -------------- |
| 128               | 128           | -        | -    | -        | 58.81/ 57.05     | 1.428M | 460M  | 158   | 65             |
| 128               | 128           | MAE      | -    | -        | 56.96            | 1.428M | 460M  | 158   | 65             |
| 128               | 128           | MTR      | -    | -        | 57.31            | 1.428M | 460M  | 158   | 65             |
| 256               | 128           | -        | -    | -        | 56.68            | 3.936M | 1365M | 121   | 31             |
| 256               | 128           | -        | -    | GAF*3    | 57.65            | 3.417M | 1194M | 96    | 35             |
| 256               | 128           | -        | -    | FGAF*3   | 57.77            | 3.417M | 1194M | 91    | 37             |
| 256               | 128           | MTR      | -    | FGAF*3   |                  |        |       |       |                |
| 256               | 128           | -        | -    | AF*3     | 54.72            | 3.932M | 1359M | 104   | 30             |
| 256               | 128           | -        | -    | FAF*3    | 58.69            | 3.932M | 1359M | 95    | 29             |
| 256               | 128           | -        | -    | FGAF*4   | 54.19            | 4.035M | 1390M | 84    | 30             |
| 256               | 128           | -        | -    | FGAF*2   | 55.68            | 2.800M | 997M  | 104   | 45             |
| 128               | 128           | -        | -    | AF(BN)   | ~~50.86~~  56.61 | 1.426M | 457M  | 161   | 64             |
| 128               | 128           | -        | -    | FAF(BN)  | ~~52.24~~  44.40 | 1.424M | 456M  | 143   | 66             |
| 128               | 128           | -        | -    | GAF(BN)  | ~~51.34~~  55.22 | 1.303M | 418M  | 147   | 64             |
| 128               | 128           | -        | -    | FGAF(BN) | ~~49.08~~  54.85 | 1.304M | 418M  | 143   | 65             |
| 256               | 128           | MTR200   | -    | FGAF_C2  |                  | 3.202M | 1026M |       |                |

测速数据

| model        | lasot result auc | FPS（10900K 1thread FPS） | FPS（10900K multi-thread FPS） |
| ------------ | ---------------- | ------------------------- | ------------------------------ |
| HCAT         |                  |                           |                                |
| LightTrack   |                  | 25                        | 59                             |
| FEAR         |                  | 16                        | 47                             |
| E.T.Track    |                  |                           |                                |
| mixformerv2  |                  |                           |                                |
| ATOM         |                  |                           |                                |
| ECO          |                  |                           |                                |
| OSTrack-256  |                  | 2.64                      | 13.3                           |
| SeqTrack-256 |                  | 0.73                      | 4.06                           |
| ARTrack-256  |                  | 1.37                      | 6.89                           |
| GRM-256      |                  | 1.75                      | 8.31                           |
| mixformer    |                  | 2.29                      | 9.12                           |
| Sim-B/16     |                  |                           |                                |
| STARK-ST50   |                  | 3.63                      | 13.22                          |
|              |                  |                           |                                |
|              |                  |                           |                                |
|              |                  |                           |                                |
|              |                  |                           |                                |
|              |                  |                           |                                |
|              |                  |                           |                                |
|              |                  |                           |                                |

无预训练情况下，参数量的增加不会带来性能明显提升

CPU速度：手机测速，取较慢的设备

比现有的效果明显提升的实验，体现改的优势

层数：2345消融实验

lasot lasot_ext tnl2k测试

1.256 AF 点

2.合适的设备做测速（手机芯片）

3.MTR预训练

长时间运行性能下降（耗电量）

CPU测速几个线程：1，2，4线程数，多任务同时运行，同时运行很多个tracker，卷积对单线程更友好，transformer对多线程更好

pytorch/onnx比较

VIT-L不用预训练59lasot

------

头部参数量高，计算量低

onnx导出，MNN进行每层时间测量

基于transformer的目标跟踪算法在精度方面取得了巨大的成功。
然而由于他们的速度较慢，限制了这些算法在边缘设备以及CPU上的应用。
对于目标跟踪任务，实际场景可能需要同时运行很多跟踪器，因此跟踪器在CPU单线程上的速度很重要
在这篇论文中，我们提出了一个新的轻量化目标跟踪网络，叫做EfficientTrack。
它将transformer block进行分组，降低了计算量和参数量。
此外，针对轻量化目标跟踪算法，预训练仍然重要。
我们还提出了针对于目标跟踪算法的一种预训练方式，叫做Mask autoencoder for TRacking(MTR)
他可以进一步提升目标跟踪算法的精度。
基于这种方法，EfficientTrack在速度和精度方面做出了很好的trade-off。
例如，EfficientTrack在CPU单线程即可达到30FPS，同时可以达到57.77AUC在lasot数据集上。
code will be available.



1. cls_token
2. 1d cos pos_embed 2d intepolate
3. conv*4 patch_embed
4. 



VitTrack speed test

| depth        | channels | params  | macs    | FPS(10900K多线程) | lasot |
| ------------ | -------- | ------- | ------- | ----------------- | ----- |
| 3            | 128      | 2.689M  | 753M    | 122               |       |
| 3            | 256      | 5.478M  | 1.648G  | 84                |       |
| 6            | 256      | 7.848M  | 2.405G  | 60                |       |
| 12           | 256      | 12.586M | 3.918G  | 39                |       |
| 4（vit-xs）  | 768      | 36.379M | 11.897G | 24                | 64.9  |
| 6            | 768      | 50.555M | 16.431G | 17                |       |
| 12(vit_base) | 768      | 93.082M | 30.032G | 10                | 70.0  |
|              |          |         |         |                   |       |
| HiT-base     |          | 42.14M  | 4.34G   | 33-37.8           | 64.6  |
| HiT-small    |          | 11.03M  | 1.13G   | 72-69.3           | 60.5  |
|              |          |         |         | 87.7              |       |
| mixformerv2  |          | 16.035M | 4.404G  | 58                | 60.6  |
| LiteTrackB4  |          | 28.329M | 7.327G  | 31                | 62.5  |
| LiteTrackB6  |          | 38.961M | 10.086G | 22                | 64.6  |
| LiteTrackB8  |          | 49.593M | 12.807G | 18                | 66.4  |
| LiteTrackB9  |          | 54.910M | 14.167G | 16                | 67.0  |
| HCAT         |          |         |         | 49                | 59.3  |
| FEAR         |          | 1.37M   | 476M    | 44                | 53.5  |

备注：默认采用单模板，分辨率为256、128，搜索区域4倍采样，模板2倍采样

备注：mixformerv2采用双模板，分辨率为224、112

备注：HiT速度为论文中的9900K上的速度



vittrack实验结果记录

| pretrain    | depth        | channels | params  | macs    | FPS(10900K多线程) | lasot |
| ----------- | ------------ | -------- | ------- | ------- | ----------------- | ----- |
|             | 3            | 128      | 2.689M  | 753M    | 122               |       |
|             | 3            | 256      | 5.478M  | 1.648G  | 84                |       |
|             | 6            | 256      | 7.848M  | 2.405G  | 60                |       |
| -           | 12           | 256      | 12.586M | 3.918G  | 39                | 60.23 |
| MTR-320     | 12           | 256      | 12.586M | 3.918G  | 39                | 62.86 |
| MTR-320-lr1 | 12           | 256      | 12.586M | 3.918G  | 39                | 63.22 |
| MTR-lr1     | 12           | 256      | 12.586M | 3.918G  | 39                | 63.08 |
|             | 12           | 256      | 12.586M | 3.918G  | 39                |       |
|             | 4（vit-xs）  | 768      | 36.379M | 11.897G | 24                | 64.9  |
|             | 6            | 768      | 50.555M | 16.431G | 17                |       |
|             | 12(vit_base) | 768      | 93.082M | 30.032G | 10                | 70.0  |

备注：无预训练情况下backbone的学习率不降低

模型训练时corner头比center头波动剧烈

关于batch_size与learning_rate：2023.11.3师兄：影响不是很大

|                | batch_size | learning_rate |      |
| -------------- | ---------- | ------------- | ---- |
| OSTrack        | 32*4=128   | 4e-4          | 300  |
| EfficientTrack | 32*1=32    | 4e-4          | 300  |
| VT             | 16*8=128   | 1e-4          | 500  |
| vittrack       | 64*2=128   | 1e-4          | 500  |

zoomTrack：对图片进行非均匀缩放，把更可能是目标的地方放的更大，带来了性能提升。给OSTrack提升了1.1个点（lasot）提升精度的trick



0-320 seed未设置（完全随机）

320开始 seed=0 328-2350挂

327开始 seed=0 329-3050挂

328开始 seed=0 330-1550挂

329开始 seed=0 330-2300挂（不到一轮）

pred[8, 317]的768数据全部是nan

已解决，半精度训练出现数据越界



模型测速实验：

模型只包含backbone，测试CPU对并行度的敏感程度，输入图像分辨率：256 + 128

10900K： core i9 10c20t

| device | model   | macs    | batch_size | FPS  |
| ------ | ------- | ------- | ---------- | ---- |
| 10900K | VIT-B   | 27.477G | 1          | 11.4 |
|        | VIT-B   |         | 2          | 12.0 |
|        | VIT-B   |         | 4          | 11.9 |
|        | d12c256 | 3.176G  | 1          | 45   |
|        | d12c256 |         | 2          | 49   |
|        | d12c256 |         | 4          | 51   |
|        | d12c256 |         | 8          | 52   |
|        | d12c256 |         | 16         | 48   |

to be done: epoch test MTR_500-LR1/MTR_500-LR01



CKA相似度分析：

完全随机数：0.38-0.51~0.53

MTR320-MTR320_LR1: 0.466-0.601-lasot 63.22

MTR320-MTR320_LR01:0.635-0.731-lasot 62.58

MTR500-MTR500_LR1:0.465-0.609-lasot 63.08

MTR500-MTR500_LR01:0.599-0.711-lasot 62.80

MTR500-nopretrain:0.662-0.769-lasot 60.23

|                   | epochs | lasot | trackingnet | got10k |
| ----------------- | ------ | ----- | ----------- | ------ |
| HiT-Base          | 1500   | 64.6  | 80.0        | 64.0   |
| HiT-Base          | 500    | 63.7  | 78.9        | 65.4   |
| vittrack-mtr_lr01 | 500    | 62.80 | 79.25       | 64.9   |
| vittrack-mtr_lr1  | 500    | 63.08 | 79.28       | 65.2   |
| vittrack-mtr_lr1  | 480    | 62.76 | 79.11       | 65.6   |

vittrack + standard patch embed

vittrack + FPN

#### 大模型sota对比实验？

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |
|      |      |      |
|      |      |      |

#### 小模型sota对比实验

|            |      |      |
| ---------- | ---- | ---- |
| d12c256    |      |      |
| d3c256     |      |      |
| HiT        |      |      |
| HCAT       |      |      |
| LiteTrack  |      |      |
| FEAR       |      |      |
| E.T.Track  |      |      |
| LightTrack |      |      |

#### 预训练对比实验

|                     |      |      |
| ------------------- | ---- | ---- |
| MAE                 |      |      |
| videoMAE            |      |      |
| trackMAE(mixformer) |      |      |
| MTR                 |      |      |

#### MTR实验部分表格（mask rate消融实验）

备注：本实验表格为VIT-Base模型，采用256-256分辨率，4-4倍采样，头部采用cornerv3，预训练1亿张图片，batchsize=64，finetune 500 epoch，backbone衰减为0.1倍的结果。

| mask rate | average   | Lasot | Lasot ext | trackingnet | got10k |
| --------- | --------- | ----- | --------- | ----------- | ------ |
| 50        | -         | 69.4  | 49.5      | -           | 73.5   |
| 55        | 69.175    | 69.8  | 49.8      | 83.2        | 73.9   |
| 60        | **69.25** | 69.9  | 50.0      | 83.5        | 73.6   |
| 65        | 69.05     | 69.8  | 49.3      | 83.4        | 73.7   |
| 70        | 69.075    | 70.3  | 49.0      | 83.3        | 73.7   |
| 75        | 68.425    | 69.5  | 48.5      | 83.2        | 72.5   |
| 80        | 68.7      | 69.2  | 48.5      | 83.2        | 73.9   |
| 85        | 68.1      | 68.7  | 48.3      | 82.9        | 72.5   |

图像分类更关注图像全局特征，而tracking任务不仅需要全局特征，也需要局部特征，因此mask rate相比于MAE选择的更低一些。

#### MTR实验部分表格（预训练数据集消融实验）

备注：本实验表格为VIT-Base模型，采用256-256分辨率，4-4倍采样，头部采用cornerv3，预训练1亿张图片，batchsize=16*8，finetune 500 epoch，backbone衰减为0.1倍的结果。

| datasets          | average | Lasot | Lasot ext | trackingnet | got10k |
| ----------------- | ------- | ----- | --------- | ----------- | ------ |
| None              |         |       |           |             |        |
| imn1k             |         | 67.9  |           | 82          | 70     |
| 4*tracking+imn1k  |         | 69.8  | 49.9      |             | 73     |
| 4*tracking+imn22k |         | 69.1  | 48.8      |             | 73.4   |

为什么imagenet22k图像更多效果却变差了？

#### 小模型patch_embedding部分消融实验表格

备注：本实验表格采用12层256通道的小型VIT作为主干网络，学习率不衰减的结果

| patch_embedding | pretrain | average | Lasot       | trackingnet  | got10k     |
| --------------- | -------- | ------- | ----------- | ------------ | ---------- |
| conv16*16       | -        |         |             |              |            |
| conv2*2         | -        |         | 60.23       | 76.61        | 61.2       |
| conv16*16       | MTR      |         | 62.38(-0.7) | 78.72(-0.56) | 63.8(-1.4) |
| conv2*2         | MTR      |         | 63.08       | 79.28        | 65.2       |

与MAT的区别：

MAT输入模板和搜索区域，训练前加载MAE权重，训练数据仅包括tracking的几个数据集，训练的时间也非常短，6400x500epoch图像对。finetune6400x300epoch

模型采用VIT作backbone，然后再用depthwise correlation作特征交互。MAT解码部分搜索区域和模板分别恢复，模板部分恢复搜索区域对应位置的crop。Lasot最高结果为67.8AUC。

DropMAE:

twinMAE：K400视频数据集，两张图作输入，复原图像，效果比MAE好一点

DropMAE：twinMAE的attention图更关注图像内部，而不关注于两张图的联系，在decoder中把一张图内的attention drop掉一部分，强制模型学习两帧之间的信息，效果非常好

freeze backbone的实验对比？

#### finetune消融实验表格

|                | average | lasot | trackingnet | got10k |
| -------------- | ------- | ----- | ----------- | ------ |
| baseline       |         | 63.08 | 79.28       | 65.2   |
| backbone lr0.1 |         | 62.80 | 79.25       | 64.9   |
| FPN            |         | 63.36 | 79.00       | 65.3   |

关于小模型的一些结论：

1、关于patch embedding 

无预训练：3>2>16

有预训练：2>3>16

2、red circle? red rectangle?



| model            | average | lasot | trackingnet | got10k |
| ---------------- | ------- | ----- | ----------- | ------ |
| vittrack-mtr_lr1 | 69.19   | 63.08 | 79.28       | 65.2   |
| rect-blue        | 68.25   | 62.02 | 78.93       | 63.8   |
| learn-rect       | 68.9    | 62.45 | 78.95       | 65.3   |
|                  |         |       |             |        |
|                  |         |       |             |        |

训练集提升，测试集降低？

原图画框or template画框？

| model                  | pretrain | average      | lasot        | trackingnet | got10k     |
| ---------------------- | -------- | ------------ | ------------ | ----------- | ---------- |
| ostrack(no ce)(32*4)   | MAE      | 75.07        | 68.7         | 82.9        | 73.6       |
| baseline(64*2)         | MAE      | 75.14        | 68.53        | 83.60       | 73.3       |
| baseline(32*2)         | MAE      | 74.72        | 68.29        | 83.19       | 72.7       |
| fixed-learn-rect(64*2) | MAE      | 74.74(-0.4)  | 68.01        | 83.01       | 73.2       |
| templatecolor(64*2)    | MAE      | 75.14(+0.0)  | 68.91        | 83.11       | 73.4       |
| templateembed(32*2)    | MAE      | 75.00(+0.28) | 68.55        | 83.54       | 72.9       |
| templateembed(64*2)    | MAE      | 75.25(+0.11) | 68.95        | 83.59       | 73.2       |
| templatecolor(64*2)    | CLIP     |              | 65.86        |             | 69.1       |
| search embed(64*2)     | MAE      |              |              |             | 67.3(-6)   |
| search & template draw |          |              |              |             | 62.8(-11)  |
| jittered draw          |          |              | 60.31(-8.22) | 80.70(-2.9) | 71.9(-1.4) |
| jittered embedding     |          |              |              |             |            |
| td_osckpt              |          | 75.50(+0.36) | 68.99        | 83.62       | 73.9       |
| talpha_osckpt          |          | 74.68(-0.46) | 67.51        | 83.32       | 73.2       |
| osckpt对照组           |          | 75.03        | 68.55        | 83.33       | 73.2       |
| 冻结ostrack主体        |          |              |              |             |            |
| ViPT draw              |          |              |              |             |            |

crop 用原图的均值补充剩余部分。

search embed中训练过程中采用了上一帧的真值，模型过于依赖上一帧图像的真值，如果上一帧给的不准效果非常差。

jitter 运动信息采用标准搜索区域抖动的1/4强度进行

lasot测试集难度偏大，丢失后跟随上一帧框完全无法找回。所以掉点最为严重

ViPT设计方案：

https://arxiv.org/pdf/2312.03818.pdf

模板采用SAM生成mask作为第四通道输入

采用训练完成的ostrack进行运动信息加强，类似于artrack的两阶段训练。

ostrack ckpt + template draw

ostrack ckpt + alpha track

alpha通道box，采用01代表内部外部/采用可学习的两个数据表示？

alpha通道采用SAM生成mask

mask输入SAM生成

mask在每个数据集中的get_sequence_info方法中进行读取，数据形状为(1, W, H)在模型中进行使用，内部数据类型为float32类型，前景部分为1.，背景部分为0.。

20240110完成lasot的mask输入的dataset部分

20240112完成数据集传输

20240113完成alpha训练



jpeg4py.JPEG(path).decode()打开jpeg图像比cv快

0、mask初始化训练测试

1、更多tracker，包括轻量化tracker、ViPT

2、点初始化测试性能，sam生成前景，随机采样点初始化测试

3、模板抖动、模板更新











